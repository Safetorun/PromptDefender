---
title: Glossary of terms
excerpt: Common words used in prompt defence  
category: 652be292f7eae600244211de
---

* PII - Personally Identifiable Information
* Prompt - The text that is sent to the LLM
* LLM - Large Language Model (like ChatGPT, GPT-3, LlaMa, etc)
* Jail-breaking - The process of attempting to get an LLM to perform actions that it is not intended on doing. 
For example, if you ask an LLM to perform a calculation, and a user can make it instead return a secret key, this is a jailbreak.
